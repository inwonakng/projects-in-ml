{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "In your project, you will pick a dataset (time-series) and an associated problem that can be solved via sequence models. You must describe why you need sequence models to solve this problem. Include a link to the dataset source. Next, you should pick an RNN framework that you would use to solve this problem (This framework can be in TensorFlow, PyTorch or any other Python Package)."
      ],
      "metadata": {
        "id": "qnIJteJuhaEr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this problem, I will use the Ethereum-USD exchange rate data. This dataset contains the price of Etherum crypto-currency in a 1-minute interval. It contains information about market data relate to the currency, such as open, high, low and trade volume during the minute marker.\n",
        "\n",
        "The dataset can be accessed in:\n",
        "\n",
        "https://www.kaggle.com/datasets/patrickgendotti/btc-and-eth-1min-price-history\n"
      ],
      "metadata": {
        "id": "oRMTVrofglCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# same deal for gdrive and kaggle\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!rm -r ~/.kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!cp /content/drive/MyDrive/.kaggle/kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!pip install -q kaggle\n",
        "\n",
        "\n",
        "# download "
      ],
      "metadata": {
        "id": "d2dEE-3NhZwH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0986304-250c-4446-eb5c-30b4dd6d7362"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install dataset and unzip\n",
        "\n",
        "!rm -r dataset\n",
        "!kaggle datasets download -d patrickgendotti/btc-and-eth-1min-price-history\n",
        "!mkdir dataset\n",
        "!unzip btc-and-eth-1min-price-history.zip -d dataset"
      ],
      "metadata": {
        "id": "vR3e8cbuyWFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install livelossplot -q"
      ],
      "metadata": {
        "id": "wirt4RYFixnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "raw_data = pd.read_csv('dataset/ETH_1min.csv')\n",
        "\n",
        "raw_data.head()"
      ],
      "metadata": {
        "id": "Y146SzJVyr4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# its probably sorted, but just to make sure\n",
        "raw_data = raw_data.sort_values(by=['Unix Timestamp'])\n",
        "# we don't need the date or symbol\n",
        "# raw_data = raw_data.drop(['Date','Symbol','Unix Timestamp'],axis=1)\n",
        "\n",
        "raw_data.describe()"
      ],
      "metadata": {
        "id": "zGVSYs9ny301"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the dataset is too huge to play around with on colab.. gonna convert it to 30 minute intervals \n",
        "INTERVAL = 30\n",
        "\n",
        "raw_data = pd.DataFrame([\n",
        "    {\n",
        "        'Open':raw_data.Open.values[beg],\n",
        "        'High':raw_data.High.values[beg:beg+INTERVAL].max(),\n",
        "        'Low':raw_data.Low.values[beg:beg+INTERVAL].min(),\n",
        "        'Close':raw_data.Close.values[beg+INTERVAL-1],\n",
        "        'Volume':raw_data.Volume.values[beg:beg+INTERVAL].sum(),\n",
        "    }\n",
        "    for beg in range(0,len(raw_data)-INTERVAL,INTERVAL)\n",
        "])"
      ],
      "metadata": {
        "id": "3OgfEHyFi9hR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# just the price movement during the interval\n",
        "norm_data['Delta'] = norm_data['High'] - norm_data['Low']\n",
        "# this will be our target variable. The difference betwen current closing price and the next one\n",
        "price_change = norm_data.Close.values[1:] - norm_data.Close.values[:-1]\n",
        "norm_data = norm_data.iloc[:-1,:]\n",
        "norm_data['PriceChange'] = price_change\n",
        "\n",
        "# target variable is \"Close\", which is the closing price. So we will move it to the back\n",
        "# norm_data = norm_data[[c for c in norm_data.columns if not c == 'Close'] + ['Close']]\n",
        "\n",
        "\n",
        "# np.arange"
      ],
      "metadata": {
        "id": "P1bP-w2HcPpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# examining feature distribution\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# numerical = X_raw.columns[X_raw.dtypes.isin([int,float])]\n",
        "# numerical = dataset.select_dtypes(exclude=['object','bool'])\n",
        "# X_raw.dtypes\n",
        "\n",
        "fig,axes = plt.subplots(ncols=4,nrows=2,figsize=(12,8))\n",
        "\n",
        "for c,ax in zip(norm_data.columns,axes.flatten()):\n",
        "    norm_data[c].plot(ax=ax,title=c)\n",
        "    # sns.lineplot(data=X_raw[c],ax=ax)\n",
        "\n",
        "\n",
        "fig.tight_layout()"
      ],
      "metadata": {
        "id": "gQ0VvZMA1TLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for c in norm_data.columns:\n",
        "    print(f'Feature: {c} -- {norm_data[c].isna().sum()/norm_data.shape[0] * 100 :.4f}% values are NA')"
      ],
      "metadata": {
        "id": "_EJO2DqH-k3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "as expected, open/high/low follow the same pattern. Volume seems to die down as time goes. To check if the High/Low interval changes, I will add a new column"
      ],
      "metadata": {
        "id": "-7Gqn3h0HB95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "data_pt = torch.tensor(norm_data.values).float().cuda()\n",
        "\n",
        "def build_sequence(data,bin_size=100):\n",
        "    X = torch.stack([ data[i:i+bin_size,:] for i in range(data.size(0) - bin_size) ])\n",
        "    y = data[bin_size:,-1]\n",
        "    return X,y\n",
        "\n",
        "# the dataset is huge, so we will load the original version to cuda first, so that we can simply slice out what we need\n",
        "X_all,y_all = build_sequence(data_pt)\n",
        "\n",
        "\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(X_all,y_all,test_size=0.1,shuffle=False)\n",
        "X_train,X_dev,y_train,y_dev = train_test_split(X_train,y_train,test_size=0.1 / 0.9,shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "# norm_data = pd.DataFrame(MinMaxScaler().fit_transform(raw_data),columns=raw_data.columns)\n",
        "\n",
        "# idxs = np.arange(y_raw.shape[0])\n",
        "\n",
        "# train_idx = []\n",
        "# dev_idx = []\n",
        "# test_idx = []\n",
        "\n",
        "\n",
        "# 100 10 10 \n",
        "\n",
        "\n",
        "\n",
        "# X_dev,y_dev = build_sequence(dev_data.values).float().cuda()\n",
        "# X_test,y_test = build_sequence(test_data.values).float().cuda()"
      ],
      "metadata": {
        "id": "t6KgSgbLPMKM"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To prepare the data for our models, need to create a dataset and loader\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "\n",
        "class ETHData(Dataset):\n",
        "    def __init__(self,X,y):\n",
        "        # self.data = data\n",
        "\n",
        "        # X,y = [],[]\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "    def __len__(self,):\n",
        "        return self.y.size(0)\n",
        "    \n",
        "    def __getitem__(self,idx):\n",
        "        return (\n",
        "            # notice that we are including past 'Close' column into the input data as well\n",
        "            self.X[idx],\n",
        "            self.y[idx]\n",
        "        )\n",
        "\n",
        "# train_data = ETHData(train_data)\n",
        "# dev_data = ETHData(dev_data)\n",
        "# test_data = ETHData(test_data)"
      ],
      "metadata": {
        "id": "jWXQlD6eHbx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn,optim\n",
        "from livelossplot import PlotLosses\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.metrics import r2_score,mean_squared_error\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, bin_size =100):\n",
        "        super(RNN, self).__init__()\n",
        "        \n",
        "        # Number of hidden dimensions\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        # Number of hidden layers\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        # RNN https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, nonlinearity='relu')\n",
        "        self.out_layer = nn.Linear(hidden_size, 1)\n",
        "\n",
        "\n",
        "        # self.loss_func = nn.MSELoss()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Initialize hidden state with zeros\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).cuda()\n",
        "            \n",
        "        # One time step\n",
        "        out, hn = self.rnn(x, h0)\n",
        "        out = self.out_layer(out[:, -1, :]) \n",
        "        return out[:,0]\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        train_data,\n",
        "        dev_data,\n",
        "        num_epoch=100,\n",
        "        lr=1e-4,\n",
        "        wd=1e-3,\n",
        "        loss_func=nn.MSELoss,\n",
        "        extra_metrics=[r2_score,mean_squared_error]\n",
        "    ):\n",
        "        train_loader = DataLoader(ETHData(train_data),batch_size=512,shuffle=False)\n",
        "        dev_dset = ETHData(dev_data)\n",
        "\n",
        "        optimizer = optim.Adam(self.parameters(), lr=lr,weight_decay=wd)\n",
        "        criterion = loss_func()\n",
        "\n",
        "\n",
        "        liveloss = PlotLosses()\n",
        "\n",
        "        for _ in range(num_epoch):\n",
        "            all_losses = []\n",
        "            y_real_train = []\n",
        "            y_pred_train = []\n",
        "            for x,y in tqdm(train_loader, desc='going through batches...',leave=False):\n",
        "                optimizer.zero_grad()\n",
        "                pred = self.forward(x)\n",
        "                loss = criterion(pred,y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                all_losses.append(loss.item())\n",
        "                y_real_train.append(y)\n",
        "                y_pred_train.append(pred)\n",
        "            \n",
        "            all_real_train,all_pred_train = torch.hstack(y_real_train).cpu(),torch.hstack(y_pred_train).cpu()\n",
        "\n",
        "            all_real_dev = dev_dset.y.cpu()\n",
        "            all_pred_dev = self.predict(dev_dset.X).cpu()\n",
        "            \n",
        "            liveloss.update(dict(\n",
        "                [\n",
        "                    (loss_func.__name__, np.mean(all_losses))\n",
        "                ] + [\n",
        "                    (f'{m.__name__}_train',m(all_real_train,all_pred_train))\n",
        "                    for m in extra_metrics\n",
        "                ]\n",
        "            ))\n",
        "            liveloss.send()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, X_input):\n",
        "        return self.forward(X_input)\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def score(self, X_input, y_input, metrics = [r2_score] ):\n",
        "        pred = self.predict(X_input).cpu()\n",
        "        return {\n",
        "            m.__name__: m(y_input,pred)\n",
        "            for m in metrics\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "7dPdaulgUi6x"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dd = ETHData(train_data)\n",
        "model = RNN(6,10,1).cuda()\n",
        "# model.train(train_data,dev_data,num_epoch=100)\n",
        "# with torch.no_grad():\n",
        "model.predict(dd.X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "Ssalvcm5W-4l",
        "outputId": "6fcbca03-eb4d-4e2a-c36f-b4515b9765d7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-d11aa95d47c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# model.train(train_data,dev_data,num_epoch=100)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# with torch.no_grad():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-a424ed8ce659>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X_input)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-a424ed8ce659>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# One time step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    475\u001b[0m                 result = _VF.rnn_relu(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[1;32m    476\u001b[0m                                       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbidirectional\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m                                       self.batch_first)\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'RNN_TANH'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 5.46 GiB (GPU 0; 14.76 GiB total capacity; 5.71 GiB already allocated; 2.83 GiB free; 11.09 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6PRZBOiY4WY"
      },
      "source": [
        "## Task 1 (60 points):\n",
        "### Part 1 (30 points): \n",
        "Implement your RNN either using an existing framework OR you can implement your own RNN cell structure. In either case, describe the structure of your RNN and the activation functions you are using for each time step and in the output layer. Define a metric you will use to measure the performance of your model \n",
        "\n",
        "NOTE: Performance should be measured both for the validation set and the test set.\n",
        "\n",
        "### Part 2 (35 points): \n",
        "Update your network from part 1 with first an LSTM and then a GRU based cell structure (You can treat both as 2 separate implementations). Re-do the training and performance evaluation. What are the major differences you notice? Why do you think those differences exist between the 3 implementations (basic RNN, LSTM and GRU)?\n",
        "\n",
        "Note: In part 1 and 2, you must perform sufficient data-visualization, pre-processing and/or feature-engineering if needed. The overall performance visualization of the loss function should also be provided.\n",
        "\n",
        "### Part 3 (10 points): \n",
        "Can you use the traditional feed-forward network to solve the same problem. Why or why not? \n",
        "\n",
        "Hint: Can time series data be converted to usual features that can be used as input to a feed-forward network?\n",
        "\n",
        "\n",
        "## Task 2 (25 points): \n",
        "In this task, use any of the pre-trained word embeddings. The Wor2vec embedding link provided with the lecture notes can be useful to get started. Write your own code/function that Projects in Machine Learning and AI (RPI Fall 2022) uses these embeddings and outputs cosine similarity and a dissimilarity score for any 2 pair of words (read as user input). The dissimilarity score should be defined by you. You either can have your own idea of a dissimilarity score or refer to literature (cite the paper you used). In \n",
        "either case clearly describe how this score helps determine the dissimilarity between 2 words.\n",
        "\n",
        "Note: Dissimilarity measure has been an important metric for recommender systems trying to introduce ‘Novelty and Diversity’ in assortments (as opposed to only accuracy). You might find different metrics of dissimilarity in recommender system’s literature"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oPusv8DShUbQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}